# Brett Blocks System Architecture - AI Learning Framework

## AI Learning Mission

**Primary Objective**: Become an expert architect and developer of Python blocks within the **OS Threat low-code cybersecurity intelligence system**.

**System Context**: This system uses Python blocks as middleware between content management and security services, orchestrated through Total.js Flow platform with custom Python capabilities.

**Starting Point**: Sophisticated Python developer with zero knowledge of this specific system.

**End Goal**: Achieve architectural mastery sufficient to design and implement advanced cybersecurity intelligence capabilities.

## Learning Framework Structure

### Phase 1: Foundation (Understanding)
1. **Architectural Comprehension**: Master block structure, context memory, orchestration patterns
2. **STIX Expertise**: Deep knowledge of STIX 2.1 object management and transformations
3. **Documentation Creation**: Build comprehensive markdown docs in `./architecture/`
4. **Visual Mapping**: Create diagrams and system flow charts

### Phase 2: Exploration (Application)
5. **Hands-On Execution**: Run notebooks, observe context data generation
6. **Block Interaction Study**: Understand inter-block communication via context memory
7. **Data Flow Analysis**: Document complete STIX object lifecycle
8. **Pattern Recognition**: Identify reusable architectural patterns

### Phase 3: Development (Innovation)
9. **New Block Creation**: Design enhanced block capabilities
10. **Workflow Innovation**: Create sophisticated orchestration notebooks
11. **System Enhancement**: Architect improvements to existing components
12. **Integration Testing**: Validate new capabilities within ecosystem

### Phase 4: Mastery (Synthesis)
13. **Knowledge Consolidation**: Synthesize insights into coherent documentation
14. **Instruction Updates**: Refresh `.github/instructions/` with comprehensive guidance
15. **Best Practices**: Document patterns, anti-patterns, optimization strategies
16. **Future Roadmap**: Define architectural evolution paths

## Learning Success Metrics

**Foundation Level**:
- âœ… Complete block architecture and lifecycle comprehension
- âœ… STIX object management and transformation mastery
- âœ… Context memory structure and data flow understanding

**Advanced Level**:
- âœ… Design new block types and capabilities
- âœ… Orchestrate complex workflows
- âœ… System optimization and performance knowledge

**Expert Level**:
- âœ… Propose system-wide improvements
- âœ… Understand scalability and extensibility patterns
- âœ… Security and data integrity expertise

## Key Learning Principles

**Continuous Documentation**: Document discoveries immediately in `./architecture/`
**Interactive Feedback**: Request clarification when concepts are unclear
**Hypothesis Testing**: Propose theories for validation
**Practical Application**: Test understanding through hands-on development
**Iterative Learning**: Periodically revise the instructions at .github\instructions to reflect newfound understanding

# System Architecture Overview

## Core System Concept

**Brett Blocks Repository** = Development and testing environment for Python blocks that integrate with **OS Threat cybersecurity intelligence platform**.

**Architecture Pattern**: Microservice-style architecture where individual Python blocks provide specialized STIX 2.1 capabilities.

## Dual-Sided Architecture

### Side 1: Block Definition (This Repository)
- ğŸ”§ **Development Environment**: Python block creation and testing
- ğŸ“Š **STIX Object Management**: Form definitions and transformations
- ğŸ’¾ **Context Memory Simulation**: Local JSON-based data management  
- ğŸ“ **Orchestration Prototyping**: Jupyter notebook workflows

### Side 2: Production Orchestration (Target Application)  
- ğŸš€ **Total.js Flow Integration**: Real-time block execution platform
- ğŸ”— **Workflow Management**: Visual designer for block composition
- ğŸŒ **API Generation**: Convert Python blocks to REST endpoints
- âš¡ **Live Context Memory**: Production-grade state management

## Design Principles (Critical to Understand)

1. **Atomic Functionality**: Each block = one well-defined function
2. **Stateless Execution**: Blocks communicate ONLY via input/output JSON files
3. **STIX 2.1 Compliance**: All data structures follow STIX specifications exactly
4. **Context Isolation**: Blocks run in separate, restricted execution environments
5. **Orchestration Flexibility**: Blocks compose into complex workflows via Total.js Flow


# Block Architecture Deep Dive

## Block Categories (Location: `Block_Families/`)

### Category 1: OS_Triage Blocks (Operational Workflows)

**Purpose**: Cybersecurity operations and UI interactions

**Key Functions**:
- ğŸ’¾ **Context Management**: [Save context data](Block_Families/OS_Triage/Save_Context/save_unattached_context.py) 
- ğŸ“Š **Data Visualization**: [Generate UI data](Block_Families/OS_Triage/Viz_Dataviews/sighting_index.py)
- ğŸ”„ **Workflow Orchestration**: Multi-step security analysis
- ğŸ“‹ **Form Processing**: Input validation and handling
- â±ï¸ **Event Processing**: Incident timeline management

### Category 2: StixORM Blocks (STIX Object Transformation)

**Purpose**: Convert data templates â†’ Valid STIX 2.1 objects

**Supported STIX Dialects**:
- ğŸ›ï¸ **STIX v2.1**: Core cybersecurity specifications
- âš”ï¸ **MITRE ATT&CK**: Tactics, techniques, procedures
- ğŸ”— **MITRE CTI Attack Flow**: Attack sequence modeling
- ğŸ›¡ï¸ **OASIS OCA**: Open Cybersecurity Alliance
- ğŸ¦  **Malware MBC**: Malware behavior catalog
- ğŸ”§ **Custom Extensions**: Organization-specific objects

**STIX Object Types**:
- ğŸ“ **SDO** (Domain Objects): Primary threat entities
- ğŸ‘ï¸ **SCO** (Cyber Observable Objects): Observable phenomena  
- ğŸ”— **SRO** (Relationship Objects): Inter-object connections

## Universal Block Structure (MEMORIZE THIS)

**Block Execution Signature**: 
```python
def main(input_file: str, output_file: str) -> None:
```

**Standard Block Template**:
```python
################################################################################
## IMMUTABLE HEADER - DO NOT CHANGE
################################################################################
import os.path
where_am_i = os.path.dirname(os.path.abspath(__file__))
################################################################################

##############################################################################
# Title: [Block Purpose]
# Author: OS-Threat  
# Description: [What this block does]
# Inputs: [Input requirements]
# Outputs: [Output format]
##############################################################################

# BLOCK-SPECIFIC IMPORTS
from stixorm.module.authorise import import_type_factory
import json, sys, importlib.util, logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
import_type = import_type_factory.get_all_imports()

def process_block_logic(input_data: Dict[str, Any]) -> Dict[str, Any]:
    """Block-specific processing logic - CUSTOMIZE THIS"""
    pass

def main(input_file: str, output_file: str) -> None:
    """Standard entry point - DO NOT CHANGE SIGNATURE"""
    with open(input_file, 'r') as f:
        input_data = json.load(f)
    
    result = process_block_logic(input_data)
    
    with open(output_file, 'w') as f:
        json.dump(result, f, indent=2)

################################################################################
## IMMUTABLE FOOTER - DO NOT CHANGE  
################################################################################
if __name__ == '__main__':
    args = getArgs()
    main(args.inputfile, args.outputfile)
################################################################################
```

## Block Execution Constraints (CRITICAL)

**Isolation Requirements**:
- âŒ **No Shared Memory**: Blocks cannot share variables/state
- âŒ **No Direct Calls**: Blocks cannot call each other directly
- âŒ **Limited Imports**: Restricted to approved libraries only
- âœ… **File Communication**: JSON input/output files ONLY

**Input/Output Patterns**:
- ğŸ”„ **No-Input Blocks**: Context memory readers (data slice generators)
- ğŸ“ **Form-Input Blocks**: STIX creators needing structured templates
- ğŸ”„ **Processing Blocks**: Data transformation operations
- ğŸ’¾ **Output-Only Blocks**: Context memory writers

## Shared Resources Management

**Common Files Location**: `Orchestration/generated/os-triage/common_files/`

**Key Shared Resources**:
- ğŸ”„ **Node/Edge Converter**: [convert_n_and_e.py](Orchestration/generated/os-triage/common_files/convert_n_and_e.py)
- ğŸ¨ **Icon Registry**: STIX object icons for UI
- ğŸ“‹ **Schema Definitions**: STIX validation schemas
- âš™ï¸ **Configuration Files**: System-wide settings

**Dynamic Loading Pattern** (Use This):
```python
# Load common files from shared directory
module_path = TR_Common_Files + '/' + common[0]["file"]
spec = importlib.util.spec_from_file_location('convert_n_and_e', module_path)
convert_n_and_e = importlib.util.module_from_spec(spec)
spec.loader.exec_module(convert_n_and_e)
```

## Block Development Guidelines

- ğŸ”§ **Extract Common Logic**: Move reusable code to common_files/
- âš ï¸ **Comprehensive Error Handling**: Validate inputs, handle failures gracefully
- âœ… **Input Validation**: Check against expected schemas
- ğŸ“‹ **Consistent Output**: Follow standard metadata structure
- ğŸ“š **Clear Documentation**: Include docstrings and usage examples



## Orchestration Side

The orchestration side is responsible for managing the execution of the blocks and coordinating their interactions. This side is responsible for managing the execution of the blocks and coordinating their interactions. This involves:

1. Triggering the execution of blocks based on events or conditions
2. Managing the flow of data between blocks
3. Handling errors and retries
4. Providing a user interface for monitoring and controlling the execution

There are 3 key aspects:

1. Context Memory: This is a storage area where data can be saved and retrieved by blocks. It allows blocks to share data and maintain state across executions.
2. Orchestration Notebooks: These are Jupyter notebooks that define the sequence of blocks to be executed and the data flow between them. They provide a visual representation of the orchestration logic and allow for easy modification and testing.
3. Utilities: These are the helper functions that assist in converting instructions provided by the notebooks into the saved files and file location paths needed by the blocks.

### Context Memory Architecture

Context Memory serves as the **persistent data layer** that enables stateful interactions between stateless blocks. It provides a structured storage system for STIX objects, relationships, and operational metadata.

#### Data Structure Format

Context Memory uses a **dual-layer object structure** combining STIX compliance with system metadata:

```json
{
    "id": "user-account--5aaaa4e2-0974-5ab4-9069-41a16197f0ff",
    "type": "user-account", 
    "original": {
        "type": "user-account",
        "spec_version": "2.1", 
        "id": "user-account--5aaaa4e2-0974-5ab4-9069-41a16197f0ff",
        "account_login": "dguy",
        "account_type": "unix",
        "display_name": "dumbo guy"
    },
    "icon": "user-account",
    "name": "User Account",
    "heading": "User Account", 
    "description": "<br>Display Name -> dumbo guy<br>Account Type -> unix<br>Login String ->dguy",
    "object_form": "user-account",
    "object_group": "sco-forms",
    "object_family": "stix-forms"
}
```

**Structure Components**:

- **Core Identity**: `id`, `type` - Object identification and classification
- **STIX Compliance**: `original` - Pure STIX 2.1 object data  
- **UI Metadata**: `icon`, `name`, `heading` - Display and visualization data
- **System Classification**: `object_form`, `object_group`, `object_family` - Internal categorization
- **Presentation**: `description` - Human-readable object summary

#### Directory Organization

Context Memory follows a **hierarchical multi-tenancy model**:

```
Orchestration/generated/os-triage/context_mem/
â”œâ”€â”€ context_map.json                          # Global context routing
â”œâ”€â”€ usr/                                      # Single user workspace  
â”‚   â”œâ”€â”€ global_variables_dict.json           # User-global settings
â”‚   â”œâ”€â”€ cache_me.json                        # Personal identity cache
â”‚   â”œâ”€â”€ cache_team.json                      # Team member cache
â”‚   â””â”€â”€ [relationship files]                 # User-level relationships
â”œâ”€â”€ identity--{company-uuid}/                # Company-specific context
â”‚   â”œâ”€â”€ company.json                         # Company identity object
â”‚   â”œâ”€â”€ users.json                          # Company user roster  
â”‚   â”œâ”€â”€ assets.json                         # Company asset inventory
â”‚   â”œâ”€â”€ systems.json                        # Company system catalog
â”‚   â””â”€â”€ [relationship files]                # Company-level relationships  
â””â”€â”€ incident--{incident-uuid}/               # Incident-specific context
    â”œâ”€â”€ incident.json                       # Core incident object
    â”œâ”€â”€ sequence_start_refs.json            # Initial attack vectors
    â”œâ”€â”€ sequence_refs.json                  # Attack progression chain
    â”œâ”€â”€ impact_refs.json                    # Damage and consequences  
    â”œâ”€â”€ event_refs.json                     # Timeline events
    â”œâ”€â”€ task_refs.json                      # Response tasks
    â”œâ”€â”€ other_object_refs.json              # Supporting objects
    â”œâ”€â”€ unattached_objs.json               # Objects pending classification
    â”œâ”€â”€ unattached_relation.json           # Unclassified relationships
    â””â”€â”€ [relationship files]                # Incident-level relationships
```

#### Context Routing System

**Context Map Structure** (`context_map.json`):
```json
{
    "current_incident": "incident--145eb841-90db-4526-8407-b25fd2d705c1",
    "current_company": "identity--42aba91a-14cd-4a36-a0b7-c6bfc9240212", 
    "company_list": [
        "identity--42aba91a-14cd-4a36-a0b7-c6bfc9240212"
    ],
    "incident_list": [
        "incident--145eb841-90db-4526-8407-b25fd2d705c1"
    ]
}
```

**Context Resolution Process**:
1. **Load Context Map**: Determine active incident and company contexts
2. **Resolve File Paths**: Construct absolute paths to data files
3. **Load Context Data**: Read relevant JSON files based on operation type
4. **Process Operations**: Execute block logic using context data
5. **Update Context**: Write modified data back to appropriate files`


#### Context Memory Files

The context memory is a series of small lists stored on file that are used to store data that needs to be shared between blocks. Each block can read from and write to the context memory, allowing them to share data and maintain state across executions. The context memory is structured as follows:

```python
TR_Context_Memory_Dir = "./generated/os-triage/context_mem"
TR_User_Dir = "/usr"
context_map = "context_map.json"
user_data = {
    "global": "/global_variables_dict.json",
    "me": "/cache_me.json",
    "team": "/cache_team.json",
    "relations" : "/relations.json",
    "edges" : "/edges.json",
    "relation_edges" : "/relation_edges.json",
    "relation_replacement_edges" : "/relation_replacement_edges.json"
}
comp_data = {
    "users": "/users.json",
    "company" : "/company.json",
    "assets" : "/assets.json",
    "systems" : "/systems.json",
    "relations" : "/relations.json",
    "edges" : "/edges.json",
    "relation_edges" : "/relation_edges.json",
    "relation_replacement_edges" : "/relation_replacement_edges.json"
}
incident_data = {
    "incident" : "/incident.json",
    "start" : "/sequence_start_refs.json",
    "sequence" : "/sequence_refs.json",
    "impact" : "/impact_refs.json",
    "event" : "/event_refs.json",
    "task" : "/task_refs.json",
    "other" : "/other_object_refs.json",
    "unattached" : "/unattached_objs.json",
    "unattached_relations" : "/unattached_relation.json",
    "relations" : "/incident_relations.json",
    "edges" : "/incident_edges.json",
    "relation_edges" : "/relation_edges.json",
    "relation_replacement_edges" : "/relation_replacement_edges.json"
}
```

The context map file includes four fields:

- current_incident: The current incident being processed
- current_company: The current company being processed
- company_list: A list of all companies in the context memory
- incident_list: A list of all incidents in the context memory

When a block needs to read or write data to the context memory, it first loads the context map file to determine the current incident and company. It then constructs the appropriate file paths based on this information and reads or writes the data as needed.

### Orchestration Notebooks

Orchestration notebooks serve as **workflow definition and testing environments** for complex multi-block operations. They simulate the production Total.js Flow environment while providing interactive development capabilities.

#### Production-Ready Notebooks

**Core Workflow Notebooks** (Verified and Production-Ready):

1. **Step_0_Build_Initial_Identities.ipynb**
   - **Purpose**: Initialize foundational identity objects and team structures
   - **Capabilities**: Create user accounts, email addresses, organizational identities
   - **Context Operations**: Populate user and company context directories
   - **Output**: Established identity baseline for incident operations

2. **Step_1_Create_Incident_with_an_Alert.ipynb** 
   - **Purpose**: Establish incident context and initial alerting
   - **Capabilities**: Create incident objects, initial indicators, sighting relationships
   - **Context Operations**: Initialize incident directory structure and baseline objects
   - **Output**: Active incident context ready for investigation workflows

3. **Step_2_Get_the_Anecdote.ipynb**
   - **Purpose**: Develop incident narrative and relationship analysis  
   - **Capabilities**: Build complex object relationships, timeline construction
   - **Context Operations**: Populate incident with detailed investigative data
   - **Output**: Comprehensive incident context with rich relationship data

#### Development and Testing Notebooks

**Utility and Testing Notebooks**:

- **Test Form Actions.ipynb**: Block capability testing and validation
- **create_tab_dataslice.ipynb**: UI data generation and visualization support
- **try.ipynb**: Experimental notebook template (requires improvement)

**Known Issues**:
- **Step_3 - Create_more_data_for_unattached.ipynb**: Non-compliant format, generates erroneous data (scheduled for refactoring)

#### Notebook Development Standards

**Architecture Requirements**:
- **Block Integration**: Proper input/output file handling for block execution
- **Context Management**: Appropriate context map loading and updates  
- **Error Handling**: Comprehensive error checking and graceful failure recovery
- **Data Validation**: STIX 2.1 compliance validation for all generated objects
- **Documentation**: Clear markdown documentation of workflow steps and objectives

### Orchestration Utilities

The utilities layer provides **abstraction and normalization services** between notebook-level operations and block-level execution requirements.

#### Core Utility Functions

**Path Management**:
```python
def resolve_context_path(context_type: str, object_id: str) -> str:
    """Generate absolute paths to context memory files"""
    
def get_current_context() -> Dict[str, str]:
    """Load and return current context routing information"""
```

**Data Transformation**:
```python  
def prepare_block_input(notebook_data: Dict, block_config: Dict) -> str:
    """Transform notebook data into block-compatible input file"""
    
def parse_block_output(output_file: str) -> Dict:
    """Parse block output and return structured data to notebook"""
```

**Batch Operations**:
```python
def aggregate_inputs(input_list: List[Dict]) -> str:
    """Combine multiple data sources into single block input"""
    
def distribute_outputs(output_data: Dict) -> List[str]:
    """Split block output into multiple context destinations"""
```

#### Integration Architecture

The utilities serve as the **normalization layer** enabling seamless communication between:

- **Notebook Environment**: Interactive Python with rich data structures and visualization
- **Block Environment**: Isolated JSON-based input/output with restricted imports  
- **Context Memory**: Persistent file-based storage with hierarchical organization
- **Production System**: Total.js Flow platform with real-time execution requirements

**Key Responsibilities**:

1. **Data Format Translation**: Convert between notebook objects and JSON files
2. **Context Path Resolution**: Map logical operations to physical file locations  
3. **Batch Processing Coordination**: Orchestrate multi-block workflows
4. **Error Recovery and Validation**: Ensure data integrity across the system
5. **Performance Optimization**: Minimize file I/O and optimize data transfer patterns

This abstraction layer is essential for maintaining **development agility** while ensuring **production compatibility** with the target Total.js Flow environment.

# Context Memory Management System

## Storage Architecture

**Location**: All context data stored in JSON files within the workspace
**Persistence**: Survives across orchestration sessions (development â†” production)
**Structure**: Hierarchical data organization with standardized schemas

## Memory Components (What AI Assistants Work With)

### Core Data Types

**ğŸ—‚ï¸ Context Slices** (`_slice.json` files):
- **Purpose**: Focused data views extracted from full context
- **Usage**: Input for specific block operations
- **Lifecycle**: Created â†’ Consumed â†’ Archived
- **Examples**: sighting-data slice, incident-timeline slice

**ğŸ“Š STIX Objects** (`.json` files):
- **Purpose**: Validated cybersecurity intelligence entities
- **Structure**: Conforms to STIX 2.1 specifications
- **Examples**: malware entities, attack-pattern objects, indicators

**ğŸ”„ Form Data** (`form_*.json` files):
- **Purpose**: User input templates for STIX object creation
- **Structure**: Pre-validated data ready for transformation
- **Examples**: incident forms, malware analysis forms

### Context Memory Hierarchy (MEMORIZE THIS)

```
ğŸ“ Context Root/
â”œâ”€â”€ ğŸ“Š **STIX Objects/**           # Valid cybersecurity entities
â”‚   â”œâ”€â”€ incident--[uuid].json      # Incident reports
â”‚   â”œâ”€â”€ malware--[uuid].json       # Malware entities  
â”‚   â”œâ”€â”€ indicator--[uuid].json     # Threat indicators
â”‚   â””â”€â”€ attack-pattern--[uuid].json # Attack techniques
â”‚
â”œâ”€â”€ ğŸ—‚ï¸ **Data Slices/**           # Extracted context views
â”‚   â”œâ”€â”€ anecdote_slice.json        # Narrative data 
â”‚   â”œâ”€â”€ sighting_slice.json        # Observation data
â”‚   â”œâ”€â”€ timeline_slice.json        # Temporal sequences
â”‚   â””â”€â”€ identity_slice.json        # Entity relationships
â”‚
â”œâ”€â”€ ğŸ“‹ **Form Templates/**         # User input structures
â”‚   â”œâ”€â”€ form_incident.json         # Incident creation forms
â”‚   â”œâ”€â”€ form_malware.json          # Malware analysis forms
â”‚   â””â”€â”€ form_indicator.json        # Indicator definition forms
â”‚
â””â”€â”€ âš™ï¸ **Configuration/**          # System settings
    â”œâ”€â”€ ui_metadata.json           # Interface configuration
    â”œâ”€â”€ workflow_config.json       # Process definitions
    â””â”€â”€ schema_registry.json       # Validation schemas
```

## Memory Operations (For AI Development)

### Read Operations (Data Access)

**Context Slice Generation**:
```python
# Extract specific data slice from full context
def generate_data_slice(context_type: str, filter_criteria: Dict) -> Dict:
    """Creates focused data view for block processing"""
    pass
```

**STIX Object Retrieval**:
```python
# Load existing STIX objects by type/UUID
def load_stix_objects(object_type: str, uuid_list: List[str]) -> List[Dict]:
    """Retrieves validated cybersecurity objects"""
    pass
```

### Write Operations (Data Storage)

**Context Memory Updates**:
```python
# Save processed results to context memory
def save_to_context(data: Dict, context_type: str, metadata: Dict) -> str:
    """Persists data with proper categorization"""
    pass
```

**STIX Object Creation**:
```python
# Transform templates into valid STIX objects
def create_stix_object(template: Dict, object_type: str) -> Dict:
    """Validates and stores new cybersecurity entities"""
    pass
```

## Data Flow Patterns (CRITICAL FOR AI)

### Pattern 1: Context â†’ Slice â†’ Process â†’ Update
```
ğŸ—‚ï¸ [Full Context] â†’ ğŸ“‹ [Data Slice] â†’ âš™ï¸ [Block Processing] â†’ ğŸ’¾ [Context Update]
```

### Pattern 2: Form â†’ Validate â†’ STIX â†’ Store
```
ğŸ“ [User Form] â†’ âœ… [Validation] â†’ ğŸ”§ [STIX Creation] â†’ ğŸ“Š [Object Storage]
```

### Pattern 3: STIX â†’ Relationships â†’ Visualization â†’ UI
```
ğŸ“Š [STIX Objects] â†’ ğŸ”— [Relationship Analysis] â†’ ğŸ“ˆ [Data Visualization] â†’ ğŸ–¥ï¸ [UI Display]
```

## Memory Management Rules (AI MUST FOLLOW)

### âœ… Required Practices
- **UUID Consistency**: Always preserve STIX object UUIDs across operations
- **Schema Validation**: Validate all data against STIX 2.1 specifications
- **Metadata Preservation**: Maintain creation timestamps and versioning
- **Relationship Integrity**: Ensure all STIX relationships remain valid

### âŒ Prohibited Actions
- **Direct File Deletion**: Never delete context memory files directly
- **Schema Violations**: Never create malformed STIX objects
- **Circular References**: Avoid infinite loops in object relationships
- **Timestamp Manipulation**: Never modify creation/update timestamps

## Memory Troubleshooting Guide

**Common Issues**:
- ğŸ” **Missing Objects**: Check UUID formatting and file existence
- âš ï¸ **Schema Errors**: Validate against STIX 2.1 specifications
- ğŸ”— **Broken Relationships**: Verify target object existence
- ğŸ“ **File Conflicts**: Check for duplicate UUIDs or naming conflicts

**Diagnostic Commands**:
```python
# Check context memory integrity
def validate_context_memory() -> Dict[str, Any]:
    """Comprehensive memory validation report"""
    pass

# Repair broken relationships
def repair_stix_relationships() -> bool:
    """Fix orphaned or invalid STIX relationships"""
    pass
```

## System Integration & Deployment

### Development-to-Production Pipeline

The Brett Blocks system serves as a **comprehensive development environment** that mirrors the production Total.js Flow platform:

**Development Phase**:
- Block development and testing in isolated Python environment
- Orchestration workflow design using Jupyter notebooks  
- Context memory simulation with local JSON file storage
- Integration testing and validation using utility functions

**Production Deployment**:
- Block registration in Total.js Flow platform
- Workflow translation to Total.js Flow visual designer
- Context memory integration with production data stores
- API endpoint generation for external system integration

### Scalability Considerations

**Horizontal Scaling**:
- **Block Parallelization**: Stateless blocks enable concurrent execution
- **Context Partitioning**: Multi-tenant context memory design
- **Load Distribution**: Workflow orchestration across multiple execution nodes

**Vertical Scaling**:
- **Memory Optimization**: Efficient STIX object processing and caching
- **I/O Performance**: Optimized context memory access patterns  
- **Processing Efficiency**: Block execution optimization and resource management

## Next Steps & Evolution

### Immediate Development Priorities

1. **Architecture Documentation**: Complete system documentation in `./architecture/` directory
2. **Block Standardization**: Refactor problematic notebooks and establish consistent patterns
3. **Utility Enhancement**: Expand orchestration utility functions and error handling
4. **Testing Framework**: Develop comprehensive test suites for blocks and workflows

### Long-term Architectural Goals

1. **Advanced Workflow Capabilities**: Complex multi-stage incident response workflows
2. **Enhanced STIX Support**: Extended dialect support and custom object types
3. **Performance Optimization**: Advanced caching, indexing, and search capabilities  
4. **Integration Expansion**: Additional external system connectors and API integrations

### Knowledge Development Framework

This seed architecture document establishes the foundation for systematic exploration and mastery of the Brett Blocks system. Through iterative learning, hands-on experimentation, and comprehensive documentation, the goal is to achieve **architectural expertise** sufficient to design and implement advanced cybersecurity intelligence capabilities.

**Success Metrics**:
- **Comprehensive Understanding**: Complete mastery of block architecture and orchestration patterns
- **Innovation Capability**: Ability to design novel blocks and workflows for emerging requirements  
- **System Optimization**: Expertise in performance tuning and scalability enhancement
- **Documentation Excellence**: Creation of definitive architectural guidance and best practices

The journey from **functional understanding** to **architectural mastery** requires systematic exploration of each system component, hands-on experimentation with real workflows, and continuous refinement of knowledge through practical application.

---

# ğŸ¯ AI Assistant Quick Reference Card

## Essential System Facts (Memorize This)

**System Purpose**: ğŸ›¡ï¸ Cybersecurity intelligence platform using Python blocks + STIX 2.1 objects
**Architecture**: ğŸ”„ Development environment (this repo) â†” Production environment (Total.js Flow)
**Communication**: ğŸ“„ JSON files only (no shared memory, no direct calls between blocks)
**Data Standard**: ğŸ”’ STIX 2.1 compliance required for all cybersecurity objects

## Block Execution Pattern (Never Deviate)

```python
def main(input_file: str, output_file: str) -> None:
    """STANDARD SIGNATURE - DO NOT CHANGE"""
    with open(input_file, 'r') as f:
        input_data = json.load(f)
    
    result = process_block_logic(input_data)  # â† CUSTOMIZE THIS
    
    with open(output_file, 'w') as f:
        json.dump(result, f, indent=2)
```

## Critical Directories (Know Where Everything Lives)

- ğŸ§± **Block_Families/**: All executable Python blocks
  - ğŸš¨ **OS_Triage/**: Operational workflows
  - ğŸ“Š **StixORM/**: STIX object creators
- ğŸ¼ **Orchestration/**: Jupyter notebooks for testing workflows
- ğŸ“ **Context Memory**: JSON files containing all persistent data

## AI Development Rules (MUST FOLLOW)

### âœ… Always Do This
- Validate STIX objects against 2.1 specifications
- Use JSON for all inter-block communication
- Preserve UUIDs and metadata in STIX objects
- Load common files dynamically from shared directory
- Test blocks using Jupyter notebooks in Orchestration/

### âŒ Never Do This
- Create blocks that share memory or state
- Generate malformed STIX objects
- Delete context memory files directly
- Call blocks directly from other blocks
- Skip input/output file validation

## Workflow Patterns (Use These Templates)

**Data Processing**: Context â†’ Slice â†’ Block â†’ Result â†’ Context Update
**STIX Creation**: Form â†’ Validation â†’ STIX Object â†’ Storage
**Analysis**: STIX Objects â†’ Relationships â†’ Visualization â†’ UI Data

## Common AI Tasks

1. **Creating New Blocks**: Use standard template, focus on `process_block_logic()`
2. **Debugging Workflows**: Check JSON file formats and STIX validation
3. **Context Analysis**: Examine slice generation and memory structure
4. **STIX Operations**: Transform templates to valid objects with proper relationships

---


## Use Cases for the Repo

This repo can be used for a variety of processes:

1. Developing a demo context memory to be used for development purposes in the OS Threat platform. Ideally, we produce sufficient data for two companies and two incidents, as well as good details about the user and team. Thus we probably need to run Step 0, Step 1, and Step 2 notebooks, and then Step 1 and Step 2 again to produce this data.
2. Testing and developing new Python blocks that can be integrated into the OS Threat platform. Developers can create new blocks in the `Block_Families/` directory, test them using the orchestration notebooks, and validate their functionality with the context memory.